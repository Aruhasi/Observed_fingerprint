{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GSAT trend patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In[1]:\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import cartopy.crs as ccrs\n",
    "import cartopy.feature as cfeature\n",
    "# %%\n",
    "# define function\n",
    "import src.SAT_function as data_process\n",
    "import src.Data_Preprocess as preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import src.slurm_cluster as scluster\n",
    "client, scluster = scluster.init_dask_slurm_cluster()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def func_mk(x):\n",
    "    \"\"\"\n",
    "    Mann-Kendall test for trend\n",
    "    \"\"\"\n",
    "    results = data_process.mk_test(x)\n",
    "    slope = results[0]\n",
    "    p_val = results[1]\n",
    "    return slope, p_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "dir_HadCRUT5 = '/work/mh0033/m301036/Land_surf_temp/Disentangling_OBS_SAT_trend/Figure1/'\n",
    "HadCRUT5 = xr.open_dataset(dir_HadCRUT5 + 'tas_HadCRUT5_annual_anomalies_processed.nc')\n",
    "\n",
    "dir1 ='/work/mh0033/m301036/Land_surf_temp/Disentangling_OBS_SAT_trend/Supplementary/S1/data/'\n",
    "HadCRUT5_forced = xr.open_dataset(dir1 + 'HadCRUT_Forced_signal.nc')\n",
    "HadCRUT5_internal = xr.open_dataset(dir1 + 'HadCRUT_residual.nc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HadCRUT5_forced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HadCRUT5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### define function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "variable_name = ['10yr', '20yr', '30yr', '40yr', '50yr', '60yr', '70yr']\n",
    "# define the function\n",
    "def separate_data_into_intervals(data, start_year, end_year, time_interval):\n",
    "    \"\"\"\n",
    "    This function is used to separate the data into different time intervals\n",
    "    \"\"\"\n",
    "    # create a dictionary to store the data\n",
    "    data_dict = {}\n",
    "    for i in range(len(variable_name)):\n",
    "        # calculate the start year and end year\n",
    "        start_year = time_interval[variable_name[i]][0]\n",
    "        end_year = time_interval[variable_name[i]][1]\n",
    "        # select the data\n",
    "        data_dict[variable_name[i]] = data.sel(year=slice(str(start_year), str(end_year)))\n",
    "    return data_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_interval = {\n",
    "    \"10yr\":(2013,2022),\n",
    "    \"20yr\":(2003,2022),\n",
    "    \"30yr\":(1993,2022),\n",
    "    \"40yr\":(1983,2022),\n",
    "    \"50yr\":(1973,2022),\n",
    "    \"60yr\":(1963,2022),\n",
    "    \"70yr\":(1953,2022)\n",
    "}\n",
    "start_year = 1950\n",
    "end_year   = 2022"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dict = separate_data_into_intervals(HadCRUT5, start_year, end_year, time_interval)\n",
    "forced_dict = separate_data_into_intervals(HadCRUT5_forced, start_year, end_year, time_interval)\n",
    "internal_dict = separate_data_into_intervals(HadCRUT5_internal, start_year, end_year, time_interval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1st calculate the origianl trend \n",
    "trend_dict = {}\n",
    "pvalue_dict = {}\n",
    "\n",
    "for i in range(len(variable_name)):\n",
    "    data_var = data_dict[variable_name[i]]['tas']\n",
    "    \n",
    "    slope, p_values = xr.apply_ufunc(\n",
    "        func_mk,\n",
    "        data_var,\n",
    "        input_core_dims=[[\"year\"]],\n",
    "        output_core_dims=[[], []],\n",
    "        vectorize=True,\n",
    "        dask=\"parallelized\",\n",
    "        output_dtypes=[float, float],\n",
    "        dask_gufunc_kwargs={'allow_rechunk': True}\n",
    "    )\n",
    "    trend_dict[variable_name[i]] = slope\n",
    "    pvalue_dict[variable_name[i]] = p_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trend_annual_np = {}\n",
    "pvalue_annual_np = {}\n",
    "\n",
    "for i in range(len(variable_name)):\n",
    "    trend_annual_np[variable_name[i]] = trend_dict[variable_name[i]].values\n",
    "    pvalue_annual_np[variable_name[i]] = pvalue_dict[variable_name[i]].values\n",
    "    \n",
    "trend_annual_np['10yr']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trend_annual_da = {}\n",
    "pvalue_annual_da = {}\n",
    "\n",
    "for interval, data in trend_annual_np.items():\n",
    "    trend_annual_da[interval] = xr.DataArray(data, dims=[\"lat\", \"lon\"], coords={\"lat\": data_dict[interval].lat, \"lon\": data_dict[interval].lon})\n",
    "for interval, data in pvalue_annual_np.items():\n",
    "    pvalue_annual_da[interval] = xr.DataArray(data, dims=[\"lat\", \"lon\"], coords={\"lat\": data_dict[interval].lat, \"lon\": data_dict[interval].lon})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # save the data into netcdf file\n",
    "dir_output = '/work/mh0033/m301036/Land_surf_temp/Disentangling_OBS_SAT_trend/Figure1/data_revision/'\n",
    "for interval, data in trend_annual_da.items():\n",
    "    data.to_netcdf(dir_output + 'Raw_HadCRUT5_annual_' + interval + '_trend.nc')\n",
    "    \n",
    "for interval, data in pvalue_annual_da.items():\n",
    "    data.to_netcdf(dir_output + 'Raw_HadCRUT5_annual_' + interval + '_p_value.nc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2nd calculate the forced trend\n",
    "forced_trend_dict = {}\n",
    "forced_pvalue_dict = {}\n",
    "\n",
    "for i in range(len(variable_name)):\n",
    "    data_var = forced_dict[variable_name[i]]['tas']\n",
    "    \n",
    "    slope, p_values = xr.apply_ufunc(\n",
    "        func_mk,\n",
    "        data_var,\n",
    "        input_core_dims=[[\"year\"]],\n",
    "        output_core_dims=[[], []],\n",
    "        vectorize=True,\n",
    "        dask=\"parallelized\",\n",
    "        output_dtypes=[float, float],\n",
    "        dask_gufunc_kwargs={'allow_rechunk': True}\n",
    "    )\n",
    "    forced_trend_dict[variable_name[i]] = slope\n",
    "    forced_pvalue_dict[variable_name[i]] = p_values\n",
    "\n",
    "forced_trend_annual_np = {}\n",
    "forced_pvalue_annual_np = {}\n",
    "\n",
    "for i in range(len(variable_name)):\n",
    "    forced_trend_annual_np[variable_name[i]] = forced_trend_dict[variable_name[i]].values\n",
    "    forced_pvalue_annual_np[variable_name[i]] = forced_pvalue_dict[variable_name[i]].values\n",
    "    \n",
    "forced_trend_annual_da = {}\n",
    "forced_pvalue_annual_da = {}\n",
    "\n",
    "for interval, data in forced_trend_annual_np.items():\n",
    "    forced_trend_annual_da[interval] = xr.DataArray(data, dims=[\"lat\", \"lon\"], coords={\"lat\": data_dict[interval].lat, \"lon\": data_dict[interval].lon})\n",
    "for interval, data in forced_pvalue_annual_np.items():\n",
    "    forced_pvalue_annual_da[interval] = xr.DataArray(data, dims=[\"lat\", \"lon\"], coords={\"lat\": data_dict[interval].lat, \"lon\": data_dict[interval].lon})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # save the data into netcdf file\n",
    "# for interval, data in forced_trend_annual_da.items():\n",
    "#     data.to_netcdf(dir_output + 'HadCRUT5_annual_forced_' + interval + '_trend.nc')\n",
    "    \n",
    "# for interval, data in forced_pvalue_annual_da.items():\n",
    "#     data.to_netcdf(dir_output + 'HadCRUT5_annual_forced_' + interval + '_p_value.nc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3rd calculate the internal trend\n",
    "internal_trend_dict = {}\n",
    "internal_pvalue_dict = {}\n",
    "\n",
    "for i in range(len(variable_name)):\n",
    "    data_var = internal_dict[variable_name[i]]['tas']\n",
    "    \n",
    "    slope, p_values = xr.apply_ufunc(\n",
    "        func_mk,\n",
    "        data_var,\n",
    "        input_core_dims=[[\"year\"]],\n",
    "        output_core_dims=[[], []],\n",
    "        vectorize=True,\n",
    "        dask=\"parallelized\",\n",
    "        output_dtypes=[float, float],\n",
    "        dask_gufunc_kwargs={'allow_rechunk': True}\n",
    "    )\n",
    "    internal_trend_dict[variable_name[i]] = slope\n",
    "    internal_pvalue_dict[variable_name[i]] = p_values\n",
    "\n",
    "internal_trend_annual_np = {}\n",
    "internal_pvalue_annual_np = {}\n",
    "\n",
    "for i in range(len(variable_name)):\n",
    "    internal_trend_annual_np[variable_name[i]] = internal_trend_dict[variable_name[i]].values\n",
    "    internal_pvalue_annual_np[variable_name[i]] = internal_pvalue_dict[variable_name[i]].values\n",
    "    \n",
    "internal_trend_annual_da = {}\n",
    "internal_pvalue_annual_da = {}\n",
    "\n",
    "for interval, data in internal_trend_annual_np.items():\n",
    "    internal_trend_annual_da[interval] = xr.DataArray(data, dims=[\"lat\", \"lon\"], coords={\"lat\": data_dict[interval].lat, \"lon\": data_dict[interval].lon})\n",
    "for interval, data in internal_pvalue_annual_np.items():\n",
    "    internal_pvalue_annual_da[interval] = xr.DataArray(data, dims=[\"lat\", \"lon\"], coords={\"lat\": data_dict[interval].lat, \"lon\": data_dict[interval].lon})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # # save the data into netcdf file\n",
    "# # dir_output = '/work/mh0033/m301036/Land_surf_temp/analyses_1850_2100/Manuscript_visual_schematic/1950_2022_results/Trend_data/'\n",
    "# for interval, data in internal_trend_annual_da.items():\n",
    "#     data.to_netcdf(dir_output + 'HadCRUT5_annual_internal_' + interval + '_trend.nc')\n",
    "    \n",
    "# for interval, data in internal_pvalue_annual_da.items():\n",
    "#     data.to_netcdf(dir_output + 'HadCRUT5_annual_internal_' + interval + '_p_value.nc')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting with the Robinson Projections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['figure.figsize'] = (8, 10)\n",
    "plt.rcParams['font.size'] = 16\n",
    "# plt.rcParams['font.family'] = 'sans-serif'\n",
    "plt.rcParams['axes.labelsize'] = 16\n",
    "plt.rcParams['ytick.direction'] = 'out'\n",
    "plt.rcParams['ytick.minor.visible'] = True\n",
    "plt.rcParams['ytick.major.right'] = True\n",
    "plt.rcParams['ytick.right'] = True\n",
    "plt.rcParams['xtick.bottom'] = True\n",
    "plt.rcParams['savefig.dpi'] = 300\n",
    "plt.rcParams['savefig.bbox'] = 'tight'\n",
    "plt.rcParams['savefig.pad_inches'] = 0.1\n",
    "plt.rcParams['savefig.transparent'] = True\n",
    "\n",
    "import cartopy.crs as ccrs\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as colors\n",
    "import matplotlib.ticker as mticker\n",
    "import cartopy.feature as cfeature\n",
    "import cartopy.mpl.ticker as cticker\n",
    "import matplotlib.patches as mpatches\n",
    "import matplotlib.lines as mlines\n",
    "import matplotlib.gridspec as gridspec\n",
    "import matplotlib as mpl\n",
    "import seaborn as sns\n",
    "from matplotlib.colors import ListedColormap\n",
    "from matplotlib.colors import BoundaryNorm, ListedColormap\n",
    "\n",
    "def plot_trend_with_significance(trend_data, lats, lons, p_values, GMST_p_values=None, levels=None, extend=None, cmap=None, \n",
    "                                 title=\"\", ax=None, show_xticks=False, show_yticks=False):\n",
    "    \"\"\"\n",
    "    Plot the trend spatial pattern using Robinson projection with significance overlaid.\n",
    "\n",
    "    Parameters:\n",
    "    - trend_data: 2D numpy array with the trend values.\n",
    "    - lats, lons: 1D arrays of latitudes and longitudes.\n",
    "    - p_values: 2D array with p-values for each grid point.\n",
    "    - GMST_p_values: 2D array with GMST p-values for each grid point.\n",
    "    - title: Title for the plot.\n",
    "    - ax: Existing axis to plot on. If None, a new axis will be created.\n",
    "    - show_xticks, show_yticks: Boolean flags to show x and y axis ticks.\n",
    "    \n",
    "    Returns:\n",
    "    - contour_obj: The contour object from the plot.\n",
    "    \"\"\"\n",
    "\n",
    "    # Create a new figure/axis if none is provided\n",
    "    if ax is None:\n",
    "        fig, ax = plt.subplots(figsize=(20, 15), subplot_kw={'projection': ccrs.Robinson()})\n",
    "        ax.set_global()\n",
    "  \n",
    "    # Determine significance mask (where p-values are less than 0.05)\n",
    "    # significance_mask = p_values < 0.05\n",
    "    insignificance_mask = p_values >= 0.05\n",
    "    \n",
    "    # Plotting\n",
    "    # contour_obj = ax.pcolormesh(lons, lats, trend_data,  cmap='RdBu_r',vmin=-5.0, vmax=5.0, transform=ccrs.PlateCarree(central_longitude=180), shading='auto')\n",
    "    contour_obj = ax.contourf(lons, lats, trend_data, levels=levels, extend=extend, cmap=cmap, transform=ccrs.PlateCarree(central_longitude=0))\n",
    "\n",
    "    # Plot significance masks with different hatches\n",
    "    ax.contourf(lons, lats, insignificance_mask, levels=[0, 0.05, 1.0], hatches=[None,'///'], colors='none', edgecolor = 'white', transform=ccrs.PlateCarree())\n",
    "\n",
    "    ax.coastlines(resolution='110m')\n",
    "    gl = ax.gridlines(draw_labels=True, dms=True, x_inline=False, y_inline=False,\n",
    "                      color='gray', alpha=0.35, linestyle='--')\n",
    "\n",
    "    # Disable labels on the top and right of the plot\n",
    "    gl.top_labels = False\n",
    "    gl.right_labels = False\n",
    "\n",
    "    # Enable labels on the bottom and left of the plot\n",
    "    gl.bottom_labels = show_xticks\n",
    "    gl.left_labels = show_yticks\n",
    "    gl.xformatter = cticker.LongitudeFormatter()\n",
    "    gl.yformatter = cticker.LatitudeFormatter()\n",
    "    gl.xlabel_style = {'size': 16}\n",
    "    gl.ylabel_style = {'size': 16}\n",
    "    \n",
    "    if show_xticks:\n",
    "        gl.bottom_labels = True\n",
    "    if show_yticks:\n",
    "        gl.left_labels = True\n",
    "    \n",
    "    # ax.set_title(title, loc='center', fontsize=18, pad=5.0)\n",
    "\n",
    "    return contour_obj\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define an asymmetric colormap\n",
    "from matplotlib.colors import LinearSegmentedColormap, Normalize\n",
    "from matplotlib.colors import BoundaryNorm\n",
    "import cartopy.util as cutil\n",
    "import seaborn as sns\n",
    "import matplotlib.colors as mcolors\n",
    "import palettable\n",
    "\n",
    "intervals = [-0.2, -0.15, -0.1, -0.05, 0, 0.05, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.9, 1.1, 1.3]\n",
    "\n",
    "# Normalizing the intervals to [0, 1]\n",
    "min_interval = min(intervals)\n",
    "max_interval = max(intervals)\n",
    "normalized_intervals = [(val - min_interval) / (max_interval - min_interval) for val in intervals]\n",
    "\n",
    "# cmap = mcolors.ListedColormap(palettable.scientific.diverging.Vik_20.mpl_colors)\n",
    "cmap=mcolors.ListedColormap(palettable.cmocean.diverging.Balance_20.mpl_colors)\n",
    "\n",
    "# Define the colors at each interval\n",
    "colors = [(0.00784313725490196, 0.2, 0.4627450980392157, 1.0),\n",
    "    (0.00784313725490196, 0.2, 0.4627450980392157, 1.0),\n",
    "    (0.023529411764705882, 0.32941176470588235, 0.5450980392156862, 1.0),\n",
    "    (0.023529411764705882, 0.32941176470588235, 0.5450980392156862, 1.0),\n",
    "    (1.0, 1.0, 1.0, 1.0),\n",
    "    (1.0, 0.9, 0.98, 1.0),\n",
    "    (1.0, 0.8, 0.5, 1.0),\n",
    "    (1.0, 0.803921568627451, 0.607843137254902, 1.0), \n",
    "    (1.0, 0.6000000000000001, 0.20000000000000018, 1.0),\n",
    "    (1.0, 0.4039215686274509, 0.0, 1.0),\n",
    "    (0.8999999999999999, 0.19999999999999996, 0.0, 1.0),\n",
    "    (0.7470588235294118, 0.0, 0.0, 1.0), \n",
    "    (0.6000000000000001, 0.0, 0.0, 1.0),\n",
    "    (0.44705882352941173, 0.0, 0.0, 1.0),\n",
    "    (0.30000000000000004, 0.0, 0.0, 1.0),\n",
    "    (0.14705882352941177, 0.0, 0.0, 1.0),\n",
    "    (0.0, 0.0, 0.0, 1.0)]\n",
    "\n",
    "# Creating a list of tuples with normalized positions and corresponding colors\n",
    "color_list = list(zip(normalized_intervals, colors))\n",
    "\n",
    "# Create the colormap\n",
    "custom_cmap = LinearSegmentedColormap.from_list('my_custom_cmap', color_list)\n",
    "\n",
    "# Create a normalization\n",
    "norm = Normalize(vmin=min_interval, vmax=max_interval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform the trend data unit to degree per decade\n",
    "for i in range(len(variable_name)):\n",
    "    trend_annual_da[variable_name[i]] = trend_annual_da[variable_name[i]] * 10\n",
    "    forced_trend_annual_da[variable_name[i]] = forced_trend_annual_da[variable_name[i]] * 10\n",
    "    internal_trend_annual_da[variable_name[i]] = internal_trend_annual_da[variable_name[i]] * 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the min and max value of the trend\n",
    "for i in range(len(variable_name)):\n",
    "    print(variable_name[i], trend_annual_da[variable_name[i]].min().values, trend_annual_da[variable_name[i]].max().values)\n",
    "    print(variable_name[i], forced_trend_annual_da[variable_name[i]].min().values, forced_trend_annual_da[variable_name[i]].max().values)\n",
    "    print(variable_name[i], internal_trend_annual_da[variable_name[i]].min().values, internal_trend_annual_da[variable_name[i]].max().values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lat = internal_trend_annual_da['10yr']['lat'].values\n",
    "lon = internal_trend_annual_da['10yr']['lon'].values\n",
    "\n",
    "\n",
    "titles_rows = [\"a. 10yr(2013-2022)\", \"b. 20yr(2003-2022)\", \"c. 30yr(1993-2022)\", \"d. 40yr(1983-2022)\", \"e. 50yr(1973-2022)\", \n",
    "                  \"f. 60yr(1963-2022)\", \"g. 70yr(1953-2022)\"]\n",
    "\n",
    "titles = [\"Raw\", \"Forced wrt. MMEM\", \"Unforced wrt. MMEM\"] # only show the title for the first row\n",
    "# plot the Raw forced and unforced trend for each time interval\n",
    "# 7 rows and 3 columns\n",
    "\"\"\"\n",
    "with each row representing a time interval;\n",
    "and each column representing the raw, forced and unforced trend respectively\n",
    "\"\"\"\n",
    "\n",
    "levels = np.arange(-1.0, 1.1, 0.1)\n",
    "\n",
    "# Define the GridSpec\n",
    "fig = plt.figure(figsize=(15, 25))\n",
    "gs = gridspec.GridSpec(7, 3, figure=fig, wspace=0.01, hspace=0.1)\n",
    "\n",
    "periods = [\"10yr\", \"20yr\", \"30yr\", \"40yr\", \"50yr\", \"60yr\", \"70yr\"]\n",
    "extend = 'both'\n",
    "axes = {}\n",
    "for i, period in enumerate(periods):\n",
    "    for j in range(3):\n",
    "        is_left = j==0\n",
    "        is_bottom_row = i>=6\n",
    "        ax = fig.add_subplot(gs[i, j], projection=ccrs.Robinson(180))\n",
    "        ax.set_global()\n",
    "        \n",
    "        axes[(i,j)]=ax\n",
    "        # if i == 0:\n",
    "            # ax.text(0.5, 1.25, titles[j], va='bottom', ha='center', rotation='horizontal', fontsize=16, \n",
    "                    # weight='bold',transform=ax.transAxes)\n",
    "            \n",
    "        if j == 0:\n",
    "            trend_data = trend_annual_da[period]\n",
    "            trend_with_cyclic, lon_with_cyclic = cutil.add_cyclic_point(trend_data, coord=lon)\n",
    "            p_values = pvalue_annual_da[period]\n",
    "            p_values_with_cyclic, lon_with_cyclic = cutil.add_cyclic_point(p_values, coord=lon)\n",
    "    \n",
    "            contour_obj = plot_trend_with_significance(trend_with_cyclic, lat, lon_with_cyclic, p_values_with_cyclic, \n",
    "                        GMST_p_values=None, levels=levels,extend=extend, cmap=cmap, \n",
    "                        title=\" \", ax=ax, show_xticks = is_bottom_row, show_yticks = is_left)\n",
    "        elif j == 1:\n",
    "            trend_data = forced_trend_annual_da[period]\n",
    "            trend_with_cyclic, lon_with_cyclic = cutil.add_cyclic_point(trend_data, coord=lon)\n",
    "            p_values = forced_pvalue_annual_da[period]\n",
    "            p_values_with_cyclic, lon_with_cyclic = cutil.add_cyclic_point(p_values, coord=lon)\n",
    "    \n",
    "            contour_obj1 = plot_trend_with_significance(trend_with_cyclic, lat, lon_with_cyclic, p_values_with_cyclic, \n",
    "                        GMST_p_values=None, levels=levels,extend=extend, cmap=cmap, \n",
    "                        title=\" \", ax=ax, show_xticks = is_bottom_row, show_yticks = is_left)\n",
    "        else:\n",
    "            trend_data = internal_trend_annual_da[period]\n",
    "            trend_with_cyclic, lon_with_cyclic = cutil.add_cyclic_point(trend_data, coord=lon)\n",
    "            p_values = internal_pvalue_annual_da[period]\n",
    "            p_values_with_cyclic, lon_with_cyclic = cutil.add_cyclic_point(p_values, coord=lon)\n",
    "    \n",
    "            contour_obj2 = plot_trend_with_significance(trend_with_cyclic, lat, lon_with_cyclic, p_values_with_cyclic, \n",
    "                        GMST_p_values=None, levels=levels,extend=extend, cmap=cmap, \n",
    "                        title=\" \", ax=ax, show_xticks = is_bottom_row, show_yticks = is_left)\n",
    "# add the title for each row\n",
    "# for i, period in enumerate(periods):\n",
    "#     axes[i,0].text(0.125, 1.1, titles_rows[i], va='bottom', ha='center', rotation='horizontal', fontsize=16, \n",
    "#                     weight='bold',transform=axes[i, 0].transAxes)\n",
    "\n",
    "# Add horizontal colorbars\n",
    "cbar_ax = fig.add_axes([0.2, 0.08, 0.6, 0.01])\n",
    "cbar = plt.colorbar(contour_obj, cax=cbar_ax, orientation='horizontal', extend=extend)\n",
    "fig.text(0.5, 0.06,'Annual SAT trend ($^\\circ$C/decade)',fontsize=22, ha='center', va='bottom')\n",
    "cbar.ax.tick_params(labelsize=18)\n",
    "\n",
    "plt.tight_layout()\n",
    "fig.savefig('Segemented-HadCRUT5-OLS-trend-pattern-separation-with-Non-sig95%.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client.close()\n",
    "scluster.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "My Kernel",
   "language": "python",
   "name": "my-kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
